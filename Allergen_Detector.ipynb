{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742aaa67",
   "metadata": {},
   "source": [
    "# Does this have nuts in it?\n",
    "\n",
    "The CDC estimates that that 1-2% of Americans are afflicted with a nut allergy, and I'm one of them. Specifically, I am allergic to walnuts, pecans, hazelnuts, macadamia nuts, and brazil nuts.\n",
    "\n",
    "After living with a nut allergy for several decades, I've developed a sixth sense for detecting when nuts are likely to be present in a particular recipe, but I still miss things.\n",
    "\n",
    "Accordingly, I wanted to see if I could train a model to predict whether a recipe contains one of my allergens based on the recipe name alone.\n",
    "\n",
    "Data was sourced from Kaggle. I started with a set of Epicurious recipes but then needed to expand the set to improve the model's performance.\n",
    "* https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions\n",
    "\n",
    "* https://www.kaggle.com/datasets/paultimothymooney/recipenlg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2daae35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf882c",
   "metadata": {},
   "source": [
    "I start by importing the data, which was sourced from Kaggle. It's a set of 13,501 Epicurious recipes.\n",
    "\n",
    "*Source: https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae51674d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "      <td>Pat chicken dry with paper towels, season all ...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "      <td>Preheat oven to 400°F and line a rimmed baking...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "      <td>Place a rack in middle of oven; preheat to 400...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "      <td>Preheat oven to 350°F with rack in middle. Gen...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "      <td>Stir together brown sugar and hot water in a c...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>Brownie Pudding Cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "      <td>Preheat the oven to 350°F. Into a bowl sift to...</td>\n",
       "      <td>brownie-pudding-cake-14408</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>Israeli Couscous with Roasted Butternut Squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "      <td>Preheat oven to 475°F.\\nHalve lemons and scoop...</td>\n",
       "      <td>israeli-couscous-with-roasted-butternut-squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>Rice with Soy-Glazed Bonito Flakes and Sesame ...</td>\n",
       "      <td>['Leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "      <td>If using katsuo bushi flakes from package, moi...</td>\n",
       "      <td>rice-with-soy-glazed-bonito-flakes-and-sesame-...</td>\n",
       "      <td>['Leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>Spanakopita</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "      <td>Melt 1 tablespoon butter in a 12-inch heavy sk...</td>\n",
       "      <td>spanakopita-107344</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>Mexican Poblano, Spinach, and Black Bean \"Lasa...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "      <td>Lay 4 chiles on their sides on racks of gas bu...</td>\n",
       "      <td>mexican-poblano-spinach-and-black-bean-lasagne...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13501 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1                        Crispy Salt and Pepper Potatoes   \n",
       "2                            Thanksgiving Mac and Cheese   \n",
       "3                     Italian Sausage and Bread Stuffing   \n",
       "4                                           Newton's Law   \n",
       "...                                                  ...   \n",
       "13496                               Brownie Pudding Cake   \n",
       "13497  Israeli Couscous with Roasted Butternut Squash...   \n",
       "13498  Rice with Soy-Glazed Bonito Flakes and Sesame ...   \n",
       "13499                                        Spanakopita   \n",
       "13500  Mexican Poblano, Spinach, and Black Bean \"Lasa...   \n",
       "\n",
       "                                             Ingredients  \\\n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...   \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
       "3      ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
       "...                                                  ...   \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...   \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...   \n",
       "13498  ['Leftover katsuo bushi (dried bonito flakes) ...   \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...   \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...   \n",
       "\n",
       "                                            Instructions  \\\n",
       "0      Pat chicken dry with paper towels, season all ...   \n",
       "1      Preheat oven to 400°F and line a rimmed baking...   \n",
       "2      Place a rack in middle of oven; preheat to 400...   \n",
       "3      Preheat oven to 350°F with rack in middle. Gen...   \n",
       "4      Stir together brown sugar and hot water in a c...   \n",
       "...                                                  ...   \n",
       "13496  Preheat the oven to 350°F. Into a bowl sift to...   \n",
       "13497  Preheat oven to 475°F.\\nHalve lemons and scoop...   \n",
       "13498  If using katsuo bushi flakes from package, moi...   \n",
       "13499  Melt 1 tablespoon butter in a 12-inch heavy sk...   \n",
       "13500  Lay 4 chiles on their sides on racks of gas bu...   \n",
       "\n",
       "                                              Image_Name  \\\n",
       "0      miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1             crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2             thanksgiving-mac-and-cheese-erick-williams   \n",
       "3              italian-sausage-and-bread-stuffing-240559   \n",
       "4                     newtons-law-apple-bourbon-cocktail   \n",
       "...                                                  ...   \n",
       "13496                         brownie-pudding-cake-14408   \n",
       "13497  israeli-couscous-with-roasted-butternut-squash...   \n",
       "13498  rice-with-soy-glazed-bonito-flakes-and-sesame-...   \n",
       "13499                                 spanakopita-107344   \n",
       "13500  mexican-poblano-spinach-and-black-bean-lasagne...   \n",
       "\n",
       "                                     Cleaned_Ingredients  \n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...  \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
       "3      ['1 (¾- to 1-pound) round Italian loaf, cut in...  \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n",
       "...                                                  ...  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...  \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...  \n",
       "13498  ['Leftover katsuo bushi (dried bonito flakes) ...  \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...  \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...  \n",
       "\n",
       "[13501 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv('recipes.csv', index_col=0)\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e062e",
   "metadata": {},
   "source": [
    "A few of the columns are unnecessary. I take the two we need and make everything lowercase for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5176b6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>miso-butter roast chicken with acorn squash pa...</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crispy salt and pepper potatoes</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanksgiving mac and cheese</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian sausage and bread stuffing</td>\n",
       "      <td>['1 (¾- to 1-pound) round italian loaf, cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>newton's law</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>brownie pudding cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>israeli couscous with roasted butternut squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>rice with soy-glazed bonito flakes and sesame ...</td>\n",
       "      <td>['leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>spanakopita</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>mexican poblano, spinach, and black bean \"lasa...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      miso-butter roast chicken with acorn squash pa...   \n",
       "1                        crispy salt and pepper potatoes   \n",
       "2                            thanksgiving mac and cheese   \n",
       "3                     italian sausage and bread stuffing   \n",
       "4                                           newton's law   \n",
       "...                                                  ...   \n",
       "13496                               brownie pudding cake   \n",
       "13497  israeli couscous with roasted butternut squash...   \n",
       "13498  rice with soy-glazed bonito flakes and sesame ...   \n",
       "13499                                        spanakopita   \n",
       "13500  mexican poblano, spinach, and black bean \"lasa...   \n",
       "\n",
       "                                             ingredients  \n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...  \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
       "3      ['1 (¾- to 1-pound) round italian loaf, cut in...  \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n",
       "...                                                  ...  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...  \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...  \n",
       "13498  ['leftover katsuo bushi (dried bonito flakes) ...  \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...  \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...  \n",
       "\n",
       "[13501 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw[['Title', 'Cleaned_Ingredients']]\n",
    "df = df.rename(columns={'Title': 'name', 'Cleaned_Ingredients': 'ingredients'})\n",
    "df = df.apply(lambda x: x.str.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5010e6",
   "metadata": {},
   "source": [
    "I define a list of nuts to which I am allergic and a simple function to determine if any of them are present in an input string. This list is easily adjustable, which will enable the model to be flexible should I want to adapt it to a different set of allergens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10429153",
   "metadata": {},
   "outputs": [],
   "source": [
    "allergens = ['walnut', 'pecan', 'macadamia', 'hazelnut', 'brazil nut', 'wal nut']\n",
    "\n",
    "def find_allergens(string):\n",
    "    return any(word in string for word in allergens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d637fb",
   "metadata": {},
   "source": [
    "I label the data in new column 'allergen' that shows True when an allergen is present and False otherwise. One initial observation: only 825 of 13501 entries contain an allergen (~6%) so the dataset is somewhat skewed. That may present issues later that we could try to address with resampling or other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2632d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    12676\n",
       "True       825\n",
       "Name: allergen, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['allergen'] = df.ingredients.apply(lambda x: find_allergens(x))\n",
    "df.allergen.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdf642",
   "metadata": {},
   "source": [
    "A quick eyeball of the data labeled as containing allergens - looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143d9647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>allergen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>nut butter granola bars</td>\n",
       "      <td>['2 cups raw nuts (such as almonds, walnuts, p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>chocolate zucchini cake</td>\n",
       "      <td>['2 1/4 cups sifted all purpose flour', '1/2 c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>swiss chard pasta with toasted hazelnuts and p...</td>\n",
       "      <td>['¼ cup hazelnuts', '1 pound bow tie pasta (fa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>pear and hazelnut frangipane tart</td>\n",
       "      <td>['1 cup hazelnuts, toasted, loose skins rubbed...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>tahini-walnut magic shell</td>\n",
       "      <td>['¼ cup raw walnuts', '3 oz. white chocolate, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13477</th>\n",
       "      <td>frisée and endive salad with warm brussels spr...</td>\n",
       "      <td>['3 tablespoons white-wine vinegar', '2 tables...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>hazelnut-butter cookies with mini chocolate chips</td>\n",
       "      <td>['1 1/2 cups all purpose flour', '3/4 teaspoon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13492</th>\n",
       "      <td>cornmeal pancakes with honey-pecan butter</td>\n",
       "      <td>['1/2 cup (1 stick) unsalted european-style bu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13494</th>\n",
       "      <td>ginger-pecan roulade with honey-glazed pecans</td>\n",
       "      <td>['1/2 stick (1/4 cup) unsalted butter, melted,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>brownie pudding cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "62                               nut butter granola bars   \n",
       "69                               chocolate zucchini cake   \n",
       "70     swiss chard pasta with toasted hazelnuts and p...   \n",
       "81                     pear and hazelnut frangipane tart   \n",
       "103                            tahini-walnut magic shell   \n",
       "...                                                  ...   \n",
       "13477  frisée and endive salad with warm brussels spr...   \n",
       "13480  hazelnut-butter cookies with mini chocolate chips   \n",
       "13492          cornmeal pancakes with honey-pecan butter   \n",
       "13494      ginger-pecan roulade with honey-glazed pecans   \n",
       "13496                               brownie pudding cake   \n",
       "\n",
       "                                             ingredients  allergen  \n",
       "62     ['2 cups raw nuts (such as almonds, walnuts, p...      True  \n",
       "69     ['2 1/4 cups sifted all purpose flour', '1/2 c...      True  \n",
       "70     ['¼ cup hazelnuts', '1 pound bow tie pasta (fa...      True  \n",
       "81     ['1 cup hazelnuts, toasted, loose skins rubbed...      True  \n",
       "103    ['¼ cup raw walnuts', '3 oz. white chocolate, ...      True  \n",
       "...                                                  ...       ...  \n",
       "13477  ['3 tablespoons white-wine vinegar', '2 tables...      True  \n",
       "13480  ['1 1/2 cups all purpose flour', '3/4 teaspoon...      True  \n",
       "13492  ['1/2 cup (1 stick) unsalted european-style bu...      True  \n",
       "13494  ['1/2 stick (1/4 cup) unsalted butter, melted,...      True  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...      True  \n",
       "\n",
       "[825 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['allergen']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdade9",
   "metadata": {},
   "source": [
    "Now I define our variables for the model. I'd like to start with a Random Forest. Since the input feature is string-based and the algorithm requires a numerical input, I start by vectorizing the names of the recipes and then split them into train, validation, and test sets in a 60/20/20 ratio. The training data will be used to train the model, the validation data will be used to evaluate the model and subsequently tweak the parameters, and the test data will be used to evaluate the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3806ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = [str(x) for x in df.name]\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df.allergen\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=3)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89b555",
   "metadata": {},
   "source": [
    "Quick check on the sizes of the Train, Validation, and Test sets for features and labels. Looks right and in the expected 60/20/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5181b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100 2700 2701 8100 2700 2701\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], X_validation.shape[0], X_test.shape[0], y_train.shape[0], y_validation.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e9e20",
   "metadata": {},
   "source": [
    "Another quick check to ensure that the proportion of True in each y set is roughly similar to the population proportion of ~6%. Looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904059a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Pct True 5.98%\n",
      "Validation Set Pct True 6.63%\n",
      "Test Set Pct True 6.00%\n"
     ]
    }
   ],
   "source": [
    "y_train_pct = y_train.sum() / y_train.count()\n",
    "y_validation_pct = y_validation.sum() / y_validation.count()\n",
    "y_test_pct = y_test.sum() / y_test.count()\n",
    "\n",
    "print(\"Training Set Pct True %.2f%%\" % (y_train_pct*100))\n",
    "print(\"Validation Set Pct True %.2f%%\" % (y_validation_pct*100))\n",
    "print(\"Test Set Pct True %.2f%%\" % (y_test_pct*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48872a",
   "metadata": {},
   "source": [
    "Now let's create and train the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974e5058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301c53a",
   "metadata": {},
   "source": [
    "Now let's see how the model does on the training set. It's extremely accurate, which is great but might just mean an overfit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71452655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      7616\n",
      "        True       1.00      0.99      1.00       484\n",
      "\n",
      "    accuracy                           1.00      8100\n",
      "   macro avg       1.00      1.00      1.00      8100\n",
      "weighted avg       1.00      1.00      1.00      8100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predictions = rf_classifier.predict(X_train)\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64146ddd",
   "metadata": {},
   "source": [
    "Looking at the model's performance on the validation set now, the recall is quite low despite high precision, leading to a low F1 score. The accuracy of the model is still pretty high (albeit lower than the evaluation of the training set), so I wouldn't call this a drastically overfit model. We will need to raise the recall before this model will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ddea371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.98      2521\n",
      "        True       0.92      0.53      0.67       179\n",
      "\n",
      "    accuracy                           0.97      2700\n",
      "   macro avg       0.94      0.76      0.83      2700\n",
      "weighted avg       0.96      0.97      0.96      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e0757",
   "metadata": {},
   "source": [
    "Given the skewed nature of the sample, I try to solve the low recall problem by applying class weights. Since \"True\" values made up 6% of the population, I use a 1:15 class weight ratio to even the True and False values. Unfortunately, this adjustment does not help the recall issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3942e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      2521\n",
      "        True       0.93      0.49      0.64       179\n",
      "\n",
      "    accuracy                           0.96      2700\n",
      "   macro avg       0.95      0.74      0.81      2700\n",
      "weighted avg       0.96      0.96      0.96      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define class weights\n",
    "class_weights = {0:1.0, 1: 15}\n",
    "\n",
    "# Create, train, and evaluate the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(class_weight=class_weights)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e803fcd",
   "metadata": {},
   "source": [
    "Since the class weighting actually made the recall problem slightly worse, as a next step I'll try resampling. I define the combination resampling pipeline (over-sampling the Trues and under-sampling the Falses) and re-run the Random Forest. Unfortunately this only moderately improves the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f845ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      2521\n",
      "        True       0.85      0.53      0.65       179\n",
      "\n",
      "    accuracy                           0.96      2700\n",
      "   macro avg       0.91      0.76      0.81      2700\n",
      "weighted avg       0.96      0.96      0.96      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start by defining the combination resampling pipeline\n",
    "resampling_pipeline = Pipeline([\n",
    "    ('over_sampler', RandomOverSampler()),\n",
    "    ('under_sampler', RandomUnderSampler()),\n",
    "])\n",
    "\n",
    "# Apply combination resampling to the training data\n",
    "X_resampled, y_resampled = resampling_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create, train, and evaluate the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_resampled, y_resampled)\n",
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c692ad0",
   "metadata": {},
   "source": [
    "Since class weighting and resampling were minimally effective in improving recall, let's see if a more detailed / complex model might work. I use a Deep Learning model via TensorFlow with Sigmoid activation in the final layer given the binary desired output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfa0b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 08:09:50.707549: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 1s 2ms/step - loss: 0.2081\n",
      "Epoch 2/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.1160\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0796\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0590\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0432\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0256\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0246\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d94a040>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the output of the vectorizer I used earlier is a sparse matrix, I convert to a dense matrix.\n",
    "# This consumes a lot of memory but it should be fine for this amount of data\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "# Now I create and compile the model\n",
    "nn_model = Sequential([\n",
    "    Dense(units = 128, activation = 'relu'),\n",
    "    Dense(units = 64, activation = 'relu'),\n",
    "    Dense(units = 32, activation = 'relu'),\n",
    "    Dense(units = 16, activation = 'relu'),\n",
    "    Dense(units = 8, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_dense, y_train, epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7b54c",
   "metadata": {},
   "source": [
    "Similar to the random forest, the model does well on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa6b12fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 722us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      7616\n",
      "        True       0.97      1.00      0.99       484\n",
      "\n",
      "    accuracy                           1.00      8100\n",
      "   macro avg       0.99      1.00      0.99      8100\n",
      "weighted avg       1.00      1.00      1.00      8100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "train_predictions = nn_model.predict(X_train_dense)\n",
    "train_predictions = (train_predictions >0.5)\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d885244",
   "metadata": {},
   "source": [
    "Unfortunately, evaluation on the validation set shows low recall. Accuracy is still high indicating minimal overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b2b94d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 758us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.98      0.97      2521\n",
      "        True       0.63      0.53      0.58       179\n",
      "\n",
      "    accuracy                           0.95      2700\n",
      "   macro avg       0.80      0.75      0.78      2700\n",
      "weighted avg       0.94      0.95      0.95      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_validation_dense = X_validation.toarray()\n",
    "\n",
    "val_predictions = nn_model.predict(X_validation_dense)\n",
    "val_predictions = (val_predictions >0.5)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a7d24",
   "metadata": {},
   "source": [
    "It seems I'll need to add more data in order to improve the model's recall. This dataset from Kaggle (link below) contains similar type data to the original dataset.\n",
    "\n",
    "*https://www.kaggle.com/datasets/paultimothymooney/recipenlg*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e5dace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
       "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
       "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
       "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipenlg = pd.read_csv('RecipeNLG_dataset.csv', index_col=0)\n",
    "recipenlg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa3a1f",
   "metadata": {},
   "source": [
    "The new dataset includes more than 2 million entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa773bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231142"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipenlg.title.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a667bc",
   "metadata": {},
   "source": [
    "Clean up the column names and add the allergen column in line with the first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f76fd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2083920\n",
       "True      147222\n",
       "Name: allergen, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take only the columns we need and rename them to be consistent with the original dataset column names\n",
    "\n",
    "df2 = recipenlg[['title', 'NER']]\n",
    "df2 = df2.rename(columns={'title': 'name', 'NER': 'ingredients'})\n",
    "\n",
    "# Similar to the original dataset, I add a column that labels the data where an allergen is present\n",
    "# There roughly 6% of the recipes contain allergens, again consistent with the original set\n",
    "\n",
    "df2['allergen'] = df2.ingredients.apply(lambda x: find_allergens(x))\n",
    "df2.allergen.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc4627",
   "metadata": {},
   "source": [
    "Combine the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b2b08c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2244638"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.concat([df, df2])\n",
    "df_full.name.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534f1dd",
   "metadata": {},
   "source": [
    "Vectorize the recipe names and split the train / validation / test sets. Output the size of each set as we did before to ensure they are correctly split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4de3dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338685 446228 446229 1338685 446228 446229\n"
     ]
    }
   ],
   "source": [
    "X = [str(x) for x in df2.name]\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df2.allergen\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=4)\n",
    "print(X_train.shape[0], X_validation.shape[0], X_test.shape[0], y_train.shape[0], y_validation.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78b5ae",
   "metadata": {},
   "source": [
    "The proportion of Trues are roughly aligned with the broader population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b62bd796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Pct True 6.59%\n",
      "Validation Set Pct True 6.66%\n",
      "Test Set Pct True 6.56%\n"
     ]
    }
   ],
   "source": [
    "y_train_pct = y_train.sum() / y_train.count()\n",
    "y_validation_pct = y_validation.sum() / y_validation.count()\n",
    "y_test_pct = y_test.sum() / y_test.count()\n",
    "\n",
    "print(\"Training Set Pct True %.2f%%\" % (y_train_pct*100))\n",
    "print(\"Validation Set Pct True %.2f%%\" % (y_validation_pct*100))\n",
    "print(\"Test Set Pct True %.2f%%\" % (y_test_pct*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff904bd",
   "metadata": {},
   "source": [
    "Create and train the Random Forest classifier using the new dataset. Test it on the training set. Accuracy is still good. Recall has improved but still not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c36183b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.99      0.99   1250470\n",
      "        True       0.90      0.68      0.78     88215\n",
      "\n",
      "    accuracy                           0.97   1338685\n",
      "   macro avg       0.94      0.84      0.88   1338685\n",
      "weighted avg       0.97      0.97      0.97   1338685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = rf_classifier.predict(X_train)\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814b54b",
   "metadata": {},
   "source": [
    "Evaluate the Random Forest model on the validation set. Recall has drastically declined, as has precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "186ab23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.99      0.97    416489\n",
      "        True       0.67      0.37      0.48     29739\n",
      "\n",
      "    accuracy                           0.95    446228\n",
      "   macro avg       0.82      0.68      0.73    446228\n",
      "weighted avg       0.94      0.95      0.94    446228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4114f",
   "metadata": {},
   "source": [
    "Adding the new dataset did not help the recall, and the dataset is now so large that running the model takes a very long time. As a next step, instead of adding the entire new dataset, I will add True and False values in similar proportions to correct the skew of the data. I suspect this may improve recall (in addition to shrinking the set to make the model more efficient). I will add all 147,222 True values and an equal amount of randomly sampled False values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "861c6a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147222"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_true = df2[df2.allergen == True]\n",
    "df2_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868a6fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_false = df2[df2.allergen == False]\n",
    "df2_false_sample = df2_false.sample(n = 147222, random_state = 4)\n",
    "df2_false_sample.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef395ab",
   "metadata": {},
   "source": [
    "Combine these two new equal True and False datasets with the original - note that now the proportion of Trues and Falses is much more equal. Additionally, the size of the dataset (~300k) is much smaller than the last iteration (>2mm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15cb2b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307945\n",
      "148047\n",
      "159898\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.concat([df2_true, df2_false_sample, df])\n",
    "\n",
    "print(df3.shape[0])\n",
    "print(df3[df3.allergen == True].shape[0])\n",
    "print(df3[df3.allergen == False].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c8e3d",
   "metadata": {},
   "source": [
    "Once again, vectorize and split the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dfe6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [str(x) for x in df3.name]\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df3.allergen\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd60b7",
   "metadata": {},
   "source": [
    "Check that the proportion of Trues should now be a bit less than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f071c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Pct True 47.94%\n",
      "Validation Set Pct True 48.35%\n",
      "Test Set Pct True 48.20%\n"
     ]
    }
   ],
   "source": [
    "y_train_pct = y_train.sum() / y_train.count()\n",
    "y_validation_pct = y_validation.sum() / y_validation.count()\n",
    "y_test_pct = y_test.sum() / y_test.count()\n",
    "\n",
    "print(\"Training Set Pct True %.2f%%\" % (y_train_pct*100))\n",
    "print(\"Validation Set Pct True %.2f%%\" % (y_validation_pct*100))\n",
    "print(\"Test Set Pct True %.2f%%\" % (y_test_pct*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c41d54",
   "metadata": {},
   "source": [
    "Let's try the neural network first this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54c17802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5774/5774 [==============================] - 38s 6ms/step - loss: 0.3774\n",
      "Epoch 2/10\n",
      "5774/5774 [==============================] - 37s 6ms/step - loss: 0.3175\n",
      "Epoch 3/10\n",
      "5774/5774 [==============================] - 39s 7ms/step - loss: 0.2845\n",
      "Epoch 4/10\n",
      "5774/5774 [==============================] - 36s 6ms/step - loss: 0.2605\n",
      "Epoch 5/10\n",
      "5774/5774 [==============================] - 36s 6ms/step - loss: 0.2426\n",
      "Epoch 6/10\n",
      "5774/5774 [==============================] - 35s 6ms/step - loss: 0.2255\n",
      "Epoch 7/10\n",
      "5774/5774 [==============================] - 40s 7ms/step - loss: 0.2113\n",
      "Epoch 8/10\n",
      "5774/5774 [==============================] - 39s 7ms/step - loss: 0.2026\n",
      "Epoch 9/10\n",
      "5774/5774 [==============================] - 36s 6ms/step - loss: 0.1932\n",
      "Epoch 10/10\n",
      "5774/5774 [==============================] - 36s 6ms/step - loss: 0.1855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168c6a850>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "# Create and compile the model\n",
    "nn_model = Sequential([\n",
    "    Dense(units = 128, activation = 'relu'),\n",
    "    Dense(units = 64, activation = 'relu'),\n",
    "    Dense(units = 32, activation = 'relu'),\n",
    "    Dense(units = 16, activation = 'relu'),\n",
    "    Dense(units = 8, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_dense, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e1802",
   "metadata": {},
   "source": [
    "Evaluate the neural network model on the training set - finally recall and pecision are both >0.90!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24c645d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5774/5774 [==============================] - 17s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.92      0.93     96184\n",
      "        True       0.92      0.94      0.93     88583\n",
      "\n",
      "    accuracy                           0.93    184767\n",
      "   macro avg       0.93      0.93      0.93    184767\n",
      "weighted avg       0.93      0.93      0.93    184767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "train_predictions = nn_model.predict(X_train_dense)\n",
    "train_predictions = (train_predictions >0.5)\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22de23",
   "metadata": {},
   "source": [
    "Evaluate the model on the validation set. The stats aren't quite as good as the training set results, but still much improved over previous iterations. There is certainly some level of overfitting here. Given the divergence between training and validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9af6ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925/1925 [==============================] - 6s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.83      0.84     31809\n",
      "        True       0.82      0.85      0.84     29780\n",
      "\n",
      "    accuracy                           0.84     61589\n",
      "   macro avg       0.84      0.84      0.84     61589\n",
      "weighted avg       0.84      0.84      0.84     61589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_validation_dense = X_validation.toarray()\n",
    "\n",
    "val_predictions = nn_model.predict(X_validation_dense)\n",
    "val_predictions = (val_predictions >0.5)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea34a1",
   "metadata": {},
   "source": [
    "Trying a simpler model like the Random Forest may help the overfitting issue. Let's fit and then test the Random Forest model on the training set. Results look largely similar to the Neural Network (slightly better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "989ed188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.95      0.96     96184\n",
      "        True       0.94      0.97      0.96     88583\n",
      "\n",
      "    accuracy                           0.96    184767\n",
      "   macro avg       0.96      0.96      0.96    184767\n",
      "weighted avg       0.96      0.96      0.96    184767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = rf_classifier.predict(X_train)\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c51f48",
   "metadata": {},
   "source": [
    "Let's try the Random Forest now on the validation set. The results are similar to the Neural Network, with stats worse than the training set but still materially improved over previous iterations. The difference in statistics between the training and validation sets suggests some level of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f217640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.84      0.85     31809\n",
      "        True       0.83      0.86      0.84     29780\n",
      "\n",
      "    accuracy                           0.85     61589\n",
      "   macro avg       0.85      0.85      0.85     61589\n",
      "weighted avg       0.85      0.85      0.85     61589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008e0da",
   "metadata": {},
   "source": [
    "Since it still appears that the model is overfitting, let's try an even simpler model - a Logistic. I fit the model and evaluate it on the training set. Note that the model does not converge, but increasing the max iterations even very significantly does not impact the quality of the model, so I keep it at the default 100 max iterations. The Logistic model underperforms the Random Forest and Neural Network on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c14ad059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.85      0.86     96184\n",
      "        True       0.84      0.85      0.85     88583\n",
      "\n",
      "    accuracy                           0.85    184767\n",
      "   macro avg       0.85      0.85      0.85    184767\n",
      "weighted avg       0.85      0.85      0.85    184767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jason/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = logistic_model.predict(X_train)\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5a027",
   "metadata": {},
   "source": [
    "Now let's evaluate the Logistic on the validation set. The Logistic performs very similarly on the validation set compared to the training set, suggesting that the overfitting of previous models may have been solved by the simpler Logistic. The evaluatory statistics are also very similar to those of the Random Forest and Neural Network. The Logistic also took significantly less time to run versus the other models. \n",
    "\n",
    "In summary, the Logistic appears to be less (or not) overfitted, similarly effective, and much faster than the Random Forest and Neural Network. Based on this, I will select the Logistic as the optimal model for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81635e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.84      0.84     31809\n",
      "        True       0.83      0.84      0.83     29780\n",
      "\n",
      "    accuracy                           0.84     61589\n",
      "   macro avg       0.84      0.84      0.84     61589\n",
      "weighted avg       0.84      0.84      0.84     61589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions = logistic_model.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c391621",
   "metadata": {},
   "source": [
    "Now that we've chosen the model, let's evaluate it on the test set to produce an unbiased assessment of its accuracy. The results of the test set are practically identical to those of the validation set and satisfactory for my purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22dfe605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.84      0.84     31905\n",
      "        True       0.83      0.84      0.83     29684\n",
      "\n",
      "    accuracy                           0.84     61589\n",
      "   macro avg       0.84      0.84      0.84     61589\n",
      "weighted avg       0.84      0.84      0.84     61589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = logistic_model.predict(X_test)\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f516d89",
   "metadata": {},
   "source": [
    "Another benefit of the Logistic model is its simplicity and interpretability - by analyzing the coefficients we can see which words, when present, contribute the most to a prediction of allergens present. \n",
    "\n",
    "Here I pull the coefficients from the model and place them in a dataframe alongside the un-vectorized words from the input data. I print a list of the top contributors based on their coefficients - note that all of these top words contribute significantly when compared to the average coefficent, which is near zero.\n",
    "\n",
    "It's not surprising that the top words are the allergens themselves, as I would expect any recipe with an allergen in its name to contain that allergen. Some of the other top names such as \"Muhammara\" are specific dishes that frequently contain allergens (walnuts in the case of Muhammara), which is particularly useful as those would be harder for a human to detect unless they were already familiar with that dish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f47ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Summary Stats:\n",
      "count    26133.000000\n",
      "mean        -0.021752\n",
      "std          0.418596\n",
      "min         -4.190000\n",
      "25%         -0.150000\n",
      "50%          0.000000\n",
      "75%          0.020000\n",
      "max          6.100000\n",
      "Name: Coefficient, dtype: float64\n",
      "\n",
      "        Word  Coefficient\n",
      "      pecans         6.10\n",
      "     walnuts         6.10\n",
      "      walnut         5.76\n",
      "    hazelnut         5.63\n",
      "       pecan         5.33\n",
      "   hazelnuts         5.29\n",
      "    pralines         4.71\n",
      "   fruitcake         4.36\n",
      "millionaires         3.83\n",
      "     romesco         3.78\n",
      "    rugelach         3.61\n",
      "     baklava         3.61\n",
      "     turtles         3.49\n",
      "     waldorf         3.37\n",
      "     haroset         3.19\n",
      "     praline         3.19\n",
      "     lizzies         3.19\n",
      "   muhammara         3.18\n",
      "        ball         3.05\n",
      "         log         3.03\n",
      "        lush         2.99\n",
      "        nuts         2.98\n",
      "    charoset         2.97\n",
      "    whiskers         2.93\n",
      "       ozark         2.89\n",
      "       rocks         2.87\n",
      "  fruitcakes         2.82\n",
      " millionaire         2.81\n",
      "        toll         2.79\n",
      "  earthquake         2.79\n",
      "     sandies         2.76\n",
      "      turtle         2.75\n",
      "        lust         2.73\n",
      "     penuche         2.73\n",
      "    rugalach         2.73\n",
      " ladyfingers         2.72\n",
      "       wafer         2.72\n",
      "    gianduja         2.72\n",
      "       nutty         2.70\n",
      "       chews         2.66\n",
      " hummingbird         2.63\n",
      "    gianduia         2.61\n",
      "     bonbons         2.56\n",
      "    panforte         2.55\n",
      "      dukkah         2.52\n",
      "        gods         2.51\n",
      "     chipped         2.49\n",
      "       winks         2.48\n",
      "       drops         2.43\n",
      "    diamonds         2.41\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_model.coef_[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients_df = pd.DataFrame({'Word': feature_names, 'Coefficient': coefficients})\n",
    "coefficients_df.Coefficient = coefficients_df.Coefficient.apply(lambda x: round(x, 2))\n",
    "top_words = coefficients_df.sort_values(by='Coefficient', ascending=False).head(50)\n",
    "print('Coefficient Summary Stats:')\n",
    "print(coefficients_df.Coefficient.describe())\n",
    "print()\n",
    "\n",
    "print(top_words.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5c35f",
   "metadata": {},
   "source": [
    "At last we have a satisfactory model - now we can productionize the prediction model and I can start eating some food! \n",
    "\n",
    "Here I define a function to use our model to predict whether a single recipe contains allergens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64c3c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_recipe(recipe):\n",
    "    recipe_lowercase = recipe.lower()\n",
    "    vector = vectorizer.transform([recipe_lowercase])\n",
    "    prediction = logistic_model.predict(vector)    \n",
    "    return recipe + ' likely contains allergens' if prediction else recipe + ' is likely allergen-free!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82a072ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pear and Endive Salad likely contains allergens\n"
     ]
    }
   ],
   "source": [
    "tester = 'Pear and Endive Salad'\n",
    "print(test_recipe(tester))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
