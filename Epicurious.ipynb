{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daae35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae51674d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "      <td>Pat chicken dry with paper towels, season all ...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "      <td>Preheat oven to 400°F and line a rimmed baking...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "      <td>Place a rack in middle of oven; preheat to 400...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "      <td>Preheat oven to 350°F with rack in middle. Gen...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "      <td>Stir together brown sugar and hot water in a c...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>Brownie Pudding Cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "      <td>Preheat the oven to 350°F. Into a bowl sift to...</td>\n",
       "      <td>brownie-pudding-cake-14408</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>Israeli Couscous with Roasted Butternut Squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "      <td>Preheat oven to 475°F.\\nHalve lemons and scoop...</td>\n",
       "      <td>israeli-couscous-with-roasted-butternut-squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>Rice with Soy-Glazed Bonito Flakes and Sesame ...</td>\n",
       "      <td>['Leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "      <td>If using katsuo bushi flakes from package, moi...</td>\n",
       "      <td>rice-with-soy-glazed-bonito-flakes-and-sesame-...</td>\n",
       "      <td>['Leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>Spanakopita</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "      <td>Melt 1 tablespoon butter in a 12-inch heavy sk...</td>\n",
       "      <td>spanakopita-107344</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>Mexican Poblano, Spinach, and Black Bean \"Lasa...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "      <td>Lay 4 chiles on their sides on racks of gas bu...</td>\n",
       "      <td>mexican-poblano-spinach-and-black-bean-lasagne...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13501 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1                        Crispy Salt and Pepper Potatoes   \n",
       "2                            Thanksgiving Mac and Cheese   \n",
       "3                     Italian Sausage and Bread Stuffing   \n",
       "4                                           Newton's Law   \n",
       "...                                                  ...   \n",
       "13496                               Brownie Pudding Cake   \n",
       "13497  Israeli Couscous with Roasted Butternut Squash...   \n",
       "13498  Rice with Soy-Glazed Bonito Flakes and Sesame ...   \n",
       "13499                                        Spanakopita   \n",
       "13500  Mexican Poblano, Spinach, and Black Bean \"Lasa...   \n",
       "\n",
       "                                             Ingredients  \\\n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...   \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
       "3      ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
       "...                                                  ...   \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...   \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...   \n",
       "13498  ['Leftover katsuo bushi (dried bonito flakes) ...   \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...   \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...   \n",
       "\n",
       "                                            Instructions  \\\n",
       "0      Pat chicken dry with paper towels, season all ...   \n",
       "1      Preheat oven to 400°F and line a rimmed baking...   \n",
       "2      Place a rack in middle of oven; preheat to 400...   \n",
       "3      Preheat oven to 350°F with rack in middle. Gen...   \n",
       "4      Stir together brown sugar and hot water in a c...   \n",
       "...                                                  ...   \n",
       "13496  Preheat the oven to 350°F. Into a bowl sift to...   \n",
       "13497  Preheat oven to 475°F.\\nHalve lemons and scoop...   \n",
       "13498  If using katsuo bushi flakes from package, moi...   \n",
       "13499  Melt 1 tablespoon butter in a 12-inch heavy sk...   \n",
       "13500  Lay 4 chiles on their sides on racks of gas bu...   \n",
       "\n",
       "                                              Image_Name  \\\n",
       "0      miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1             crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2             thanksgiving-mac-and-cheese-erick-williams   \n",
       "3              italian-sausage-and-bread-stuffing-240559   \n",
       "4                     newtons-law-apple-bourbon-cocktail   \n",
       "...                                                  ...   \n",
       "13496                         brownie-pudding-cake-14408   \n",
       "13497  israeli-couscous-with-roasted-butternut-squash...   \n",
       "13498  rice-with-soy-glazed-bonito-flakes-and-sesame-...   \n",
       "13499                                 spanakopita-107344   \n",
       "13500  mexican-poblano-spinach-and-black-bean-lasagne...   \n",
       "\n",
       "                                     Cleaned_Ingredients  \n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...  \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
       "3      ['1 (¾- to 1-pound) round Italian loaf, cut in...  \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n",
       "...                                                  ...  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...  \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...  \n",
       "13498  ['Leftover katsuo bushi (dried bonito flakes) ...  \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...  \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...  \n",
       "\n",
       "[13501 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the csv\n",
    "\n",
    "raw = pd.read_csv('recipes.csv', index_col=0)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5176b6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>miso-butter roast chicken with acorn squash pa...</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crispy salt and pepper potatoes</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanksgiving mac and cheese</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian sausage and bread stuffing</td>\n",
       "      <td>['1 (¾- to 1-pound) round italian loaf, cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>newton's law</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>brownie pudding cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>israeli couscous with roasted butternut squash...</td>\n",
       "      <td>['1 preserved lemon', '1 1/2 pound butternut s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>rice with soy-glazed bonito flakes and sesame ...</td>\n",
       "      <td>['leftover katsuo bushi (dried bonito flakes) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>spanakopita</td>\n",
       "      <td>['1 stick (1/2 cup) plus 1 tablespoon unsalted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>mexican poblano, spinach, and black bean \"lasa...</td>\n",
       "      <td>['12 medium to large fresh poblano chiles (2 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      miso-butter roast chicken with acorn squash pa...   \n",
       "1                        crispy salt and pepper potatoes   \n",
       "2                            thanksgiving mac and cheese   \n",
       "3                     italian sausage and bread stuffing   \n",
       "4                                           newton's law   \n",
       "...                                                  ...   \n",
       "13496                               brownie pudding cake   \n",
       "13497  israeli couscous with roasted butternut squash...   \n",
       "13498  rice with soy-glazed bonito flakes and sesame ...   \n",
       "13499                                        spanakopita   \n",
       "13500  mexican poblano, spinach, and black bean \"lasa...   \n",
       "\n",
       "                                             ingredients  \n",
       "0      ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
       "1      ['2 large egg whites', '1 pound new potatoes (...  \n",
       "2      ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
       "3      ['1 (¾- to 1-pound) round italian loaf, cut in...  \n",
       "4      ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n",
       "...                                                  ...  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...  \n",
       "13497  ['1 preserved lemon', '1 1/2 pound butternut s...  \n",
       "13498  ['leftover katsuo bushi (dried bonito flakes) ...  \n",
       "13499  ['1 stick (1/2 cup) plus 1 tablespoon unsalted...  \n",
       "13500  ['12 medium to large fresh poblano chiles (2 1...  \n",
       "\n",
       "[13501 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A few of the columns are unnecessary - just taking the two we need and making everything lowercase for consistency\n",
    "df = raw[['Title', 'Cleaned_Ingredients']]\n",
    "df = df.rename(columns={'Title': 'name', 'Cleaned_Ingredients': 'ingredients'})\n",
    "df = df.apply(lambda x: x.str.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10429153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define a list of nuts to which I am allergic and a simple function to determine if any of them are\n",
    "# present in a string \n",
    "\n",
    "allergens = ['walnut', 'pecan', 'macadamia', 'hazelnut', 'brazil nut', 'wal nut']\n",
    "\n",
    "def find_allergens(string):\n",
    "    return any(word in string for word in allergens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2632d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    12676\n",
       "True       825\n",
       "Name: allergen, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label the data in new column 'allergen' that shows True when an allergen is present and False otherwise\n",
    "# On initial observation is that only 825 of 13501 entries contain an allergen (~6%) so the dataset is somewhat skewed\n",
    "# That may present issues later on that we could try to address with resampling or other methods\n",
    "\n",
    "df['allergen'] = df.ingredients.apply(lambda x: find_allergens(x))\n",
    "df.allergen.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143d9647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>allergen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>nut butter granola bars</td>\n",
       "      <td>['2 cups raw nuts (such as almonds, walnuts, p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>chocolate zucchini cake</td>\n",
       "      <td>['2 1/4 cups sifted all purpose flour', '1/2 c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>swiss chard pasta with toasted hazelnuts and p...</td>\n",
       "      <td>['¼ cup hazelnuts', '1 pound bow tie pasta (fa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>pear and hazelnut frangipane tart</td>\n",
       "      <td>['1 cup hazelnuts, toasted, loose skins rubbed...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>tahini-walnut magic shell</td>\n",
       "      <td>['¼ cup raw walnuts', '3 oz. white chocolate, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13477</th>\n",
       "      <td>frisée and endive salad with warm brussels spr...</td>\n",
       "      <td>['3 tablespoons white-wine vinegar', '2 tables...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>hazelnut-butter cookies with mini chocolate chips</td>\n",
       "      <td>['1 1/2 cups all purpose flour', '3/4 teaspoon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13492</th>\n",
       "      <td>cornmeal pancakes with honey-pecan butter</td>\n",
       "      <td>['1/2 cup (1 stick) unsalted european-style bu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13494</th>\n",
       "      <td>ginger-pecan roulade with honey-glazed pecans</td>\n",
       "      <td>['1/2 stick (1/4 cup) unsalted butter, melted,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>brownie pudding cake</td>\n",
       "      <td>['1 cup all-purpose flour', '2/3 cup unsweeten...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "62                               nut butter granola bars   \n",
       "69                               chocolate zucchini cake   \n",
       "70     swiss chard pasta with toasted hazelnuts and p...   \n",
       "81                     pear and hazelnut frangipane tart   \n",
       "103                            tahini-walnut magic shell   \n",
       "...                                                  ...   \n",
       "13477  frisée and endive salad with warm brussels spr...   \n",
       "13480  hazelnut-butter cookies with mini chocolate chips   \n",
       "13492          cornmeal pancakes with honey-pecan butter   \n",
       "13494      ginger-pecan roulade with honey-glazed pecans   \n",
       "13496                               brownie pudding cake   \n",
       "\n",
       "                                             ingredients  allergen  \n",
       "62     ['2 cups raw nuts (such as almonds, walnuts, p...      True  \n",
       "69     ['2 1/4 cups sifted all purpose flour', '1/2 c...      True  \n",
       "70     ['¼ cup hazelnuts', '1 pound bow tie pasta (fa...      True  \n",
       "81     ['1 cup hazelnuts, toasted, loose skins rubbed...      True  \n",
       "103    ['¼ cup raw walnuts', '3 oz. white chocolate, ...      True  \n",
       "...                                                  ...       ...  \n",
       "13477  ['3 tablespoons white-wine vinegar', '2 tables...      True  \n",
       "13480  ['1 1/2 cups all purpose flour', '3/4 teaspoon...      True  \n",
       "13492  ['1/2 cup (1 stick) unsalted european-style bu...      True  \n",
       "13494  ['1/2 stick (1/4 cup) unsalted butter, melted,...      True  \n",
       "13496  ['1 cup all-purpose flour', '2/3 cup unsweeten...      True  \n",
       "\n",
       "[825 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick eyeball of the data listed as containing allergens - looks reasonable\n",
    "df[df['allergen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3806ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I define our variables for the model. Since I will be using a Random Forest first, I start by vectorizing\n",
    "# the names of the recipes and then split them into train, validation, and test sets in a 60/20/20 ratio.\n",
    "# The training data will be used to train the model, the validation data will be used to evaluate the model and\n",
    "# subsequently tweak the parameters, and the test data will be used to evaluate the final model\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = [str(x) for x in df.name]\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df.allergen\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=3)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5181b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100 2700 2701 8100 2700 2701\n"
     ]
    }
   ],
   "source": [
    "# Check the sizes of X_train, X_validation, X_test, y_train, y_validation, and y_test\n",
    "\n",
    "print(X_train.shape[0], X_validation.shape[0], X_test.shape[0], y_train.shape[0], y_validation.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904059a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Pct True 5.98%\n",
      "Validation Set Pct True 6.63%\n",
      "Test Set Pct True 6.00%\n"
     ]
    }
   ],
   "source": [
    "# Check that the proportion of True in each y set is roughly similar to the population proportion of ~6%\n",
    "\n",
    "y_train_pct = y_train.sum() / y_train.count()\n",
    "y_validation_pct = y_validation.sum() / y_validation.count()\n",
    "y_test_pct = y_test.sum() / y_test.count()\n",
    "\n",
    "print(\"Training Set Pct True %.2f%%\" % (y_train_pct*100))\n",
    "print(\"Validation Set Pct True %.2f%%\" % (y_validation_pct*100))\n",
    "print(\"Test Set Pct True %.2f%%\" % (y_test_pct*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974e5058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the Random Forest Classifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef0bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we test the model, I define a function to output some key model evaluation statistics\n",
    "# After writing this line I decided to instead just use sklearn's \"classification_report\"\n",
    "\n",
    "def test_model(y_actual, y_predicted):\n",
    "    accuracy = np.mean(y_actual == y_predicted)\n",
    "    precision = precision_score(y_actual, y_predicted)\n",
    "    recall = recall_score(y_actual, y_predicted)\n",
    "    f1 = f1_score(y_actual, y_predicted)\n",
    "    print(f\"Model Accuracy: {accuracy}\")\n",
    "    print(f\"Model Precision: {precision}\")\n",
    "    print(f\"Model Recall: {recall}\")\n",
    "    print(f\"Model F1 Score: {f1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ddea371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.98      2521\n",
      "        True       0.93      0.52      0.67       179\n",
      "\n",
      "    accuracy                           0.97      2700\n",
      "   macro avg       0.95      0.76      0.82      2700\n",
      "weighted avg       0.96      0.97      0.96      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "\n",
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3942e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.98      2521\n",
      "        True       0.93      0.50      0.65       179\n",
      "\n",
      "    accuracy                           0.96      2700\n",
      "   macro avg       0.95      0.75      0.82      2700\n",
      "weighted avg       0.96      0.96      0.96      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The model has low recall, which may be a symptom of the skewed nature of the sample.\n",
    "# To attempt to correct this, I apply class weights in a 1:15 ratio since the Trues made up 6% of the population\n",
    "\n",
    "class_weights = {0:1.0, 1: 15}\n",
    "rf_classifier = RandomForestClassifier(class_weight=class_weights)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f845ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      2521\n",
      "        True       0.85      0.54      0.66       179\n",
      "\n",
      "    accuracy                           0.96      2700\n",
      "   macro avg       0.91      0.76      0.82      2700\n",
      "weighted avg       0.96      0.96      0.96      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since the class weights actually made the Recall problem worse, now I will try resampling\n",
    "\n",
    "# Start by defining the combination resampling pipeline\n",
    "resampling_pipeline = Pipeline([\n",
    "    ('over_sampler', RandomOverSampler()),\n",
    "    ('under_sampler', RandomUnderSampler()),\n",
    "])\n",
    "\n",
    "# Apply combination resampling to the training data\n",
    "X_resampled, y_resampled = resampling_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_resampled, y_resampled)\n",
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfa0b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 16:18:07.321678: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 1s 2ms/step - loss: 0.1993\n",
      "Epoch 2/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0978\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0583\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0374\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0253\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 0s 2ms/step - loss: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17c6cd1f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the resampling only improved recall slightly, let's try using Deep Learning via TensorFlow for a more complex model\n",
    "\n",
    "# Since the output of the vectorizer I used earlier is a sparse matrix, I convert to a dense matrix.\n",
    "# This consumes a lot of memory but it should be fine for this amount of data\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "# Now I create and compile the model\n",
    "nn_model = Sequential([\n",
    "    Dense(units = 128, activation = 'relu'),\n",
    "    Dense(units = 64, activation = 'relu'),\n",
    "    Dense(units = 32, activation = 'relu'),\n",
    "    Dense(units = 16, activation = 'relu'),\n",
    "    Dense(units = 8, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_dense, y_train, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b2b94d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 739us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.98      0.97      2521\n",
      "        True       0.68      0.51      0.59       179\n",
      "\n",
      "    accuracy                           0.95      2700\n",
      "   macro avg       0.82      0.75      0.78      2700\n",
      "weighted avg       0.95      0.95      0.95      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's evaluate the model using the validation set\n",
    "\n",
    "X_validation_dense = X_validation.toarray()\n",
    "\n",
    "val_predictions = nn_model.predict(X_validation_dense)\n",
    "val_predictions = (val_predictions >0.5)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5dace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
       "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
       "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
       "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the model's statistics are still not where I want them to be, I will append the dataset\n",
    "\n",
    "recipenlg = pd.read_csv('RecipeNLG_dataset.csv', index_col=0)\n",
    "recipenlg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa773bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This new dataset contains over 2 million entries\n",
    "\n",
    "recipenlg.title.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f76fd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                        ingredients\n",
       "0    No-Bake Nut Cookies  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...\n",
       "1  Jewell Ball'S Chicken  [\"beef\", \"chicken breasts\", \"cream of mushroom...\n",
       "2            Creamy Corn  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...\n",
       "3          Chicken Funny  [\"chicken\", \"chicken gravy\", \"cream of mushroo...\n",
       "4   Reeses Cups(Candy)    [\"peanut butter\", \"graham cracker crumbs\", \"bu..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take only the columns we need and rename them to be consistent with the original dataset column names\n",
    "\n",
    "df2 = recipenlg[['title', 'NER']]\n",
    "df2 = df2.rename(columns={'title': 'name', 'NER': 'ingredients'})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8c0979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2083920\n",
       "True      147222\n",
       "Name: allergen, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar to the original dataset, I add a column that labels the data where an allergen is present\n",
    "# There roughly 6% of the recipes contain allergens, again consistent with the original set\n",
    "\n",
    "df2['allergen'] = df2.ingredients.apply(lambda x: find_allergens(x))\n",
    "df2.allergen.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b2b08c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2244638"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that the two datasets are similarly formatted, let's combine them\n",
    "\n",
    "df_full = pd.concat([df, df2])\n",
    "df_full.name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de3dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's vectorize the names again and create the train / validation / test sets\n",
    "\n",
    "X = [str(x) for x in df2.name]\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df2.allergen\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda1f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338685 446228 446229 1338685 446228 446229\n"
     ]
    }
   ],
   "source": [
    "# The sizes of the sets are as expected\n",
    "\n",
    "print(X_train.shape[0], X_validation.shape[0], X_test.shape[0], y_train.shape[0], y_validation.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b62bd796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Pct True 6.59%\n",
      "Validation Set Pct True 6.66%\n",
      "Test Set Pct True 6.56%\n"
     ]
    }
   ],
   "source": [
    "# And the proportion of Trues are roughly aligned with the broader population\n",
    "\n",
    "y_train_pct = y_train.sum() / y_train.count()\n",
    "y_validation_pct = y_validation.sum() / y_validation.count()\n",
    "y_test_pct = y_test.sum() / y_test.count()\n",
    "\n",
    "print(\"Training Set Pct True %.2f%%\" % (y_train_pct*100))\n",
    "print(\"Validation Set Pct True %.2f%%\" % (y_validation_pct*100))\n",
    "print(\"Test Set Pct True %.2f%%\" % (y_test_pct*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36183b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the Random Forest Classifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e218c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.99      0.97    416489\n",
      "        True       0.67      0.37      0.48     29739\n",
      "\n",
      "    accuracy                           0.95    446228\n",
      "   macro avg       0.82      0.68      0.73    446228\n",
      "weighted avg       0.94      0.95      0.94    446228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Random Forest model on the validation set\n",
    "\n",
    "val_predictions = rf_classifier.predict(X_validation)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f63eaef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the recall is still not good (nor is precision) I need to try different methods.\n",
    "# I am going to try to create a more curated dataset that includes equal proportions of Trues and Falses \n",
    "# from the new dataset.\n",
    "\n",
    "df2_true = df2[df2.allergen == True]\n",
    "df2_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e20a075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147222"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I will randomly sample an equivalent amount of False values from the new dataset\n",
    "\n",
    "df2_false = df2[df2.allergen == False]\n",
    "df2_false_sample = df2_false.sample(n = 147222, random_state = 4)\n",
    "df2_false_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4527e1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307945\n",
      "148047\n",
      "159898\n"
     ]
    }
   ],
   "source": [
    "# I now combine these two new equal datasets with the original - note thte distribution of\n",
    "# Trues and Falses is much more similar\n",
    "\n",
    "df3 = pd.concat([df2_true, df2_false_sample, df])\n",
    "\n",
    "print(df3.shape[0])\n",
    "print(df3[df3.allergen == True].shape[0])\n",
    "print(df3[df3.allergen == False].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbe3feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, vectorize and split the sets\n",
    "\n",
    "X = [str(x) for x in df3.name]\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df3.allergen\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=4)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "636e91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Pct True 47.94%\n",
      "Validation Set Pct True 48.35%\n",
      "Test Set Pct True 48.20%\n"
     ]
    }
   ],
   "source": [
    "# Check that the proportion of Trues should now be a bit less than 50%\n",
    "\n",
    "y_train_pct = y_train.sum() / y_train.count()\n",
    "y_validation_pct = y_validation.sum() / y_validation.count()\n",
    "y_test_pct = y_test.sum() / y_test.count()\n",
    "\n",
    "print(\"Training Set Pct True %.2f%%\" % (y_train_pct*100))\n",
    "print(\"Validation Set Pct True %.2f%%\" % (y_validation_pct*100))\n",
    "print(\"Test Set Pct True %.2f%%\" % (y_test_pct*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c17802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time I'm going directly to the neural network\n",
    "\n",
    "X_train_dense = X_train.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99c1b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 13:20:48.869403: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5774/5774 [==============================] - 39s 7ms/step - loss: 0.3805\n",
      "Epoch 2/10\n",
      "5774/5774 [==============================] - 39s 7ms/step - loss: 0.3217\n",
      "Epoch 3/10\n",
      "5774/5774 [==============================] - 39s 7ms/step - loss: 0.2888\n",
      "Epoch 4/10\n",
      "5774/5774 [==============================] - 39s 7ms/step - loss: 0.2635\n",
      "Epoch 5/10\n",
      "5774/5774 [==============================] - 37s 6ms/step - loss: 0.2434\n",
      "Epoch 6/10\n",
      "5774/5774 [==============================] - 37s 6ms/step - loss: 0.2289\n",
      "Epoch 7/10\n",
      "5774/5774 [==============================] - 37s 6ms/step - loss: 0.2178\n",
      "Epoch 8/10\n",
      "5774/5774 [==============================] - 37s 6ms/step - loss: 0.2063\n",
      "Epoch 9/10\n",
      "5774/5774 [==============================] - 38s 7ms/step - loss: 0.1976\n",
      "Epoch 10/10\n",
      "5774/5774 [==============================] - 37s 6ms/step - loss: 0.1895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f393040>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I create and compile the model\n",
    "nn_model = Sequential([\n",
    "    Dense(units = 128, activation = 'relu'),\n",
    "    Dense(units = 64, activation = 'relu'),\n",
    "    Dense(units = 32, activation = 'relu'),\n",
    "    Dense(units = 16, activation = 'relu'),\n",
    "    Dense(units = 8, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_dense, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af6ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925/1925 [==============================] - 6s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.81      0.84     31809\n",
      "        True       0.81      0.86      0.84     29780\n",
      "\n",
      "    accuracy                           0.84     61589\n",
      "   macro avg       0.84      0.84      0.84     61589\n",
      "weighted avg       0.84      0.84      0.84     61589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_validation_dense = X_validation.toarray()\n",
    "\n",
    "val_predictions = nn_model.predict(X_validation_dense)\n",
    "val_predictions = (val_predictions >0.5)\n",
    "print(classification_report(y_validation, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22dfe605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925/1925 [==============================] - 6s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.81      0.84     31905\n",
      "        True       0.81      0.87      0.84     29684\n",
      "\n",
      "    accuracy                           0.84     61589\n",
      "   macro avg       0.84      0.84      0.84     61589\n",
      "weighted avg       0.84      0.84      0.84     61589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "test_predictions = nn_model.predict(X_test_dense)\n",
    "test_predictions = (test_predictions >0.5)\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64c3c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last we have a decent F1 score of 0.84 and can productionize the prediction model!\n",
    "# Define a function to use our model to predict whether a single recipe contains allergens\n",
    "\n",
    "def test_recipe(recipe):\n",
    "    recipe_lowercase = recipe.lower()\n",
    "    vector = vectorizer.transform([recipe_lowercase])\n",
    "    prediction = nn_model.predict(vector)    \n",
    "    return recipe + ' likely contains allergens' if prediction[0]> 0.5 else recipe + ' is likely allergen-free!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82a072ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "Seared Belgian Endive Salad likely contains allergens\n"
     ]
    }
   ],
   "source": [
    "tester = 'Seared Belgian Endive Salad'\n",
    "print(test_recipe(tester))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d3bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
